{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: [-10.        -10.         10.         10.         -9.3716011 -10.       ] Reward: 0.4365 E_i: 1 E: 0 RMSD:  0.5828\n",
      "WARNING:tensorflow:From /Users/in-justin.jose/.miniconda/envs/rl-virtual-screening/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py:644: _EagerTensorBase.cpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.identity instead.\n",
      "Critic Training Loss:  95868.91 Actor Training Loss:  -434.25964\n",
      "Action: [-10.         -10.          10.          10.          -9.39984512\n",
      " -10.        ] Reward: 0.2795 E_i: 2 E: 0 RMSD:  1.154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/in-justin.jose/Documents/Projects/E4R/rl-virtual-screening/rlvs/network/actor.py:57: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  actor_weights = np.array(self.actor_target.get_weights())\n",
      "/Users/in-justin.jose/Documents/Projects/E4R/rl-virtual-screening/rlvs/network/actor.py:58: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  target_weights = np.array(self.actor.get_weights())\n",
      "/Users/in-justin.jose/Documents/Projects/E4R/rl-virtual-screening/rlvs/network/critic.py:24: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  model_weights = np.array(self.critic.get_weights())\n",
      "/Users/in-justin.jose/Documents/Projects/E4R/rl-virtual-screening/rlvs/network/critic.py:25: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  target_weights = np.array(self.critic_target.get_weights())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critic Training Loss:  73069.41 Actor Training Loss:  -389.31976\n",
      "Action: [-10.         -10.          10.          10.          -9.83158016\n",
      " -10.        ] Reward: 0.1953 E_i: 3 E: 0 RMSD:  1.7079\n",
      "Critic Training Loss:  40154.434 Actor Training Loss:  -314.39502\n",
      "Action: [-10.         -10.          10.          10.          -8.82859039\n",
      " -10.        ] Reward: 0.1457 E_i: 4 E: 0 RMSD:  2.2117\n",
      "Critic Training Loss:  12206.185 Actor Training Loss:  -218.2017\n",
      "Action: [-10.         -10.          10.          10.          -8.89407063\n",
      " -10.        ] Reward: 0.1136 E_i: 5 E: 0 RMSD:  2.6724\n",
      "Critic Training Loss:  892.9417 Actor Training Loss:  -122.38237\n",
      "Action: [-10.         -10.          10.          10.          -9.60221767\n",
      " -10.        ] Reward: 0.0918 E_i: 6 E: 0 RMSD:  3.0874\n",
      "Critic Training Loss:  2902.2627 Actor Training Loss:  -48.250088\n",
      "Action: [-10.         -10.          10.          10.          -8.96396732\n",
      " -10.        ] Reward: 0.0775 E_i: 7 E: 0 RMSD:  3.4322\n",
      "Critic Training Loss:  3817.9856 Actor Training Loss:  -33.946598\n",
      "Action: [-10. -10.  10.  10. -10. -10.] Reward: 0.0677 E_i: 8 E: 0 RMSD:  3.7154\n",
      "Critic Training Loss:  467.62042 Actor Training Loss:  -75.38139\n",
      "Action: [-10. -10.  10.  10. -10. -10.] Reward: 0.0615 E_i: 9 E: 0 RMSD:  3.9192\n",
      "Critic Training Loss:  603.2126 Actor Training Loss:  -108.03612\n",
      "Action: [-10. -10.  10.  10. -10. -10.] Reward: 0.0582 E_i: 10 E: 0 RMSD:  4.0391\n",
      "Critic Training Loss:  143.04239 Actor Training Loss:  -84.70314\n",
      "Action: [-10.         -10.           9.          10.          -8.8896637\n",
      "  -9.91275215] Reward: 0.0573 E_i: 11 E: 0 RMSD:  4.0724\n",
      "Critic Training Loss:  74.08429 Actor Training Loss:  -64.24065\n",
      "Action: [-10. -10.   8.  10. -10. -10.] Reward: 0.0586 E_i: 12 E: 0 RMSD:  4.0223\n",
      "Critic Training Loss:  10.914267 Actor Training Loss:  -70.469376\n",
      "Action: [-10.         -10.           7.          10.          -9.92124462\n",
      " -10.        ] Reward: 0.0624 E_i: 13 E: 0 RMSD:  3.8864\n",
      "Critic Training Loss:  20.532007 Actor Training Loss:  -64.94948\n",
      "Action: [-10.         -10.           8.          10.          -9.88382435\n",
      " -10.        ] Reward: 0.0692 E_i: 14 E: 0 RMSD:  3.668\n",
      "Critic Training Loss:  1.4376915 Actor Training Loss:  -57.137367\n",
      "Action: [-10.          -9.           9.          10.          -9.33932018\n",
      "  -9.32868385] Reward: 0.0793 E_i: 15 E: 0 RMSD:  3.385\n",
      "Critic Training Loss:  5.5638466 Actor Training Loss:  -54.4168\n",
      "Action: [-10.         -10.           8.          10.          -9.98637962\n",
      "  -9.95059967] Reward: 0.095 E_i: 16 E: 0 RMSD:  3.0196\n",
      "Critic Training Loss:  5.331496 Actor Training Loss:  -48.293053\n",
      "Action: [-10. -10.   8.  10. -10. -10.] Reward: 0.1187 E_i: 17 E: 0 RMSD:  2.5891\n",
      "Critic Training Loss:  4.964066 Actor Training Loss:  -43.19915\n",
      "Action: [-10.         -10.           6.           9.79852962  -9.24000931\n",
      " -10.        ] Reward: 0.1537 E_i: 18 E: 0 RMSD:  2.117\n",
      "Critic Training Loss:  4.956431 Actor Training Loss:  -37.76406\n",
      "Action: [-10.        -10.          6.         10.         -8.9688139 -10.       ] Reward: 0.2083 E_i: 19 E: 0 RMSD:  1.6027\n"
     ]
    }
   ],
   "source": [
    "from rlvs.molecule_world.env import GraphEnv\n",
    "from rlvs.agents.ddpg_agent import DDPGAgentGNN\n",
    "env = GraphEnv()\n",
    "env.input_shape\n",
    "agent = DDPGAgentGNN(env)\n",
    "actions =  agent.play(10000)\n",
    "batch = agent.memory.sample(agent.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(len(batch))\n",
    "xx = [val['state'] for val in batch]\n",
    "\n",
    "# agent._critiq.action_gradients(xx, agent._actor.actor)\n",
    "\n",
    "with tf.GradientTape() as tape2:\n",
    "  action_tensor = agent._actor.actor(xx)\n",
    "\n",
    "  q = agent._critiq.critic([xx, action_tensor])\n",
    "  action_loss = -tf.reduce_mean(q)\n",
    "  action_gradient =  tape2.gradient(action_loss, agent._actor.actor.trainable_variables)         \n",
    "action_gradient, action_loss.numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlvs",
   "language": "python",
   "name": "rlvs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
